{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1eab1dd",
   "metadata": {},
   "source": [
    "\n",
    "# CAPTCHA Generator: Classical Compositor + Conditional DCGAN\n",
    "\n",
    "This notebook accepts a **string** and outputs a **CAPTCHA image** using two methods:\n",
    "1) **Classical**: sample segmented character assets + augment + compose.\n",
    "2) **cGAN**: train a conditional DCGAN to generate character sprites, then compose.\n",
    "\n",
    "**Expected data structure** (confirmed):\n",
    "```\n",
    "chars_by_class/\n",
    " ‚îú‚îÄ‚îÄ 0/\n",
    " ‚îú‚îÄ‚îÄ 1/\n",
    " ‚îú‚îÄ‚îÄ 2/\n",
    " ‚îú‚îÄ‚îÄ ...\n",
    " ‚îú‚îÄ‚îÄ a/\n",
    " ‚îÇ   ‚îú‚îÄ‚îÄ A_cap/\n",
    " ‚îÇ   ‚îî‚îÄ‚îÄ a_small/\n",
    " ‚îú‚îÄ‚îÄ b/\n",
    " ‚îÇ   ‚îú‚îÄ‚îÄ B_cap/\n",
    " ‚îÇ   ‚îî‚îÄ‚îÄ b_small/\n",
    " ‚îú‚îÄ‚îÄ j/        # no cap/small subfolders if visually identical\n",
    " ‚îî‚îÄ‚îÄ ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb553c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05bc89b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:26.416470300Z",
     "start_time": "2025-11-12T16:40:26.310288300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %pip install torch torchvision opencv-python pillow matplotlib tqdm\n",
    "\n",
    "import os, random, string\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFilter\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61966cb8",
   "metadata": {},
   "source": [
    "## 2. Scan Dataset & Build Token Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f25a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.734724700Z",
     "start_time": "2025-11-12T16:40:26.323313200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 tokens/classes.\n",
      "  0 -> 1301 images\n",
      "  1 -> 1333 images\n",
      "  2 -> 1237 images\n",
      "  3 -> 1335 images\n",
      "  4 -> 1300 images\n",
      "  5 -> 1262 images\n",
      "  6 -> 1297 images\n",
      "  7 -> 1253 images\n",
      "  8 -> 1271 images\n",
      "  9 -> 1292 images\n",
      "  A_cap -> 577 images\n",
      "  B_cap -> 659 images\n",
      "  C_cap -> 1175 images\n",
      "  D_cap -> 587 images\n",
      "  E_cap -> 537 images\n",
      "  F_cap -> 610 images\n",
      "  G_cap -> 593 images\n",
      "  H_cap -> 592 images\n",
      "  I_cap -> 863 images\n",
      "  L_cap -> 289 images\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"data\\chars_by_class\")  # main root\n",
    "\n",
    "DIGITS_DIR = DATA_ROOT / \"digits\"\n",
    "LETTERS_DIR = DATA_ROOT / \"letters\"\n",
    "\n",
    "def scan_dataset(root: Path):\n",
    "    assets: Dict[str, List[Path]] = {}\n",
    "    token_meta: Dict[str, dict] = {}\n",
    "\n",
    "    # ----- Digits -----\n",
    "    for ddir in (DIGITS_DIR.iterdir() if DIGITS_DIR.exists() else []):\n",
    "        if ddir.is_dir():\n",
    "            d = ddir.name\n",
    "            paths = [p for p in ddir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            if paths:\n",
    "                assets[d] = paths\n",
    "                token_meta[d] = {\"kind\": \"digit\", \"char\": d, \"source\": str(ddir)}\n",
    "\n",
    "    # ----- Letters -----\n",
    "    for ldir in (LETTERS_DIR.iterdir() if LETTERS_DIR.exists() else []):\n",
    "        if not ldir.is_dir():\n",
    "            continue\n",
    "        ch = ldir.name\n",
    "        cap_dir = ldir / f\"{ch.upper()}_cap\"\n",
    "        small_dir = ldir / f\"{ch}_small\"\n",
    "        files_direct = [p for p in ldir.glob(\"*\") if p.is_file() and p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "\n",
    "        if cap_dir.is_dir() and small_dir.is_dir():\n",
    "            cap_paths = [p for p in cap_dir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            sm_paths  = [p for p in small_dir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            if cap_paths:\n",
    "                token = f\"{ch.upper()}_cap\"\n",
    "                assets[token] = cap_paths\n",
    "                token_meta[token] = {\"kind\":\"letter_case\",\"char\":ch,\"case\":\"cap\",\"source\":str(cap_dir)}\n",
    "            if sm_paths:\n",
    "                token = f\"{ch}_small\"\n",
    "                assets[token] = sm_paths\n",
    "                token_meta[token] = {\"kind\":\"letter_case\",\"char\":ch,\"case\":\"small\",\"source\":str(small_dir)}\n",
    "        else:\n",
    "            if files_direct:\n",
    "                token = ch\n",
    "                assets[token] = files_direct\n",
    "                token_meta[token] = {\"kind\":\"letter_single\",\"char\":ch,\"source\":str(ldir)}\n",
    "\n",
    "    # ----- Mapping -----\n",
    "    idx_to_token = sorted(assets.keys())\n",
    "    token_to_idx = {t: i for i, t in enumerate(idx_to_token)}\n",
    "\n",
    "    print(f\"Found {len(idx_to_token)} tokens/classes.\")\n",
    "    for t in idx_to_token[:20]:\n",
    "        print(\" \", t, \"->\", len(assets[t]), \"images\")\n",
    "    if len(idx_to_token) > 20:\n",
    "        print(\" ...\")\n",
    "    return assets, token_meta, token_to_idx, idx_to_token\n",
    "\n",
    "ASSETS, TOKEN_META, TOKEN_TO_IDX, IDX_TO_TOKEN = scan_dataset(DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e1b8d",
   "metadata": {},
   "source": [
    "## 3. Utils: Image I/O & Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3eb8d2",
   "metadata": {},
   "source": [
    "### 3.1 Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee87a9c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.735723900Z",
     "start_time": "2025-11-12T16:40:27.711561800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: IMG_SIZE=48, NZ=96, NCLASS=54\n"
     ]
    }
   ],
   "source": [
    "# Global hyperparameters for cGAN training and generation\n",
    "IMG_SIZE = 48\n",
    "NC = 1          # number of channels (grayscale)\n",
    "NZ = 96         # latent dimension\n",
    "NGF = 64        # generator base filters\n",
    "NDF = 64        # discriminator base filters\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 64  # embedding dimension for class conditioning\n",
    "NCLASS = len(TOKEN_TO_IDX)  # will be set after scanning dataset\n",
    "\n",
    "print(f\"Configuration: IMG_SIZE={IMG_SIZE}, NZ={NZ}, NCLASS={NCLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa6f89d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.739741700Z",
     "start_time": "2025-11-12T16:40:27.737724500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_rgba(path: Path) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "def show_image(im: Image.Image, title=None, size=None):\n",
    "    plt.figure()\n",
    "    if size: im = im.resize(size, Image.NEAREST)\n",
    "    plt.imshow(im)\n",
    "    if title: plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def pick_token_for_char(ch: str) -> str:\n",
    "    if ch.isdigit():\n",
    "        if ch in ASSETS: return ch\n",
    "        raise KeyError(f\"No assets for digit {ch}\")\n",
    "    if ch.isalpha():\n",
    "        l = ch.lower()\n",
    "        cap = f\"{l.upper()}_cap\"\n",
    "        small = f\"{l}_small\"\n",
    "        if ch.isupper():\n",
    "            if cap in ASSETS: return cap\n",
    "            if l in ASSETS: return l\n",
    "            if small in ASSETS: return small\n",
    "        else:\n",
    "            if small in ASSETS: return small\n",
    "            if l in ASSETS: return l\n",
    "            if cap in ASSETS: return cap\n",
    "    raise KeyError(f\"No asset token for '{ch}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77377fbf",
   "metadata": {},
   "source": [
    "## 4. Classical Compositional Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c734ec57a117e921",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.817388Z",
     "start_time": "2025-11-12T16:40:27.745741200Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_padding(im: Image.Image, pad_x_ratio=0.25, pad_y_ratio=0.05, color=(255,255,255,255)):\n",
    "    \"\"\"\n",
    "    Add horizontal/vertical padding around a character image.\n",
    "    pad_x_ratio: fraction of width to pad each side (0.25 ‚Üí 25% left/right)\n",
    "    pad_y_ratio: fraction of height to pad each side (0.05 ‚Üí 5% top/bottom)\n",
    "    color: background fill (white with full alpha)\n",
    "    \"\"\"\n",
    "    w, h = im.size\n",
    "    px, py = int(w * pad_x_ratio), int(h * pad_y_ratio)\n",
    "    new_w, new_h = w + 2 * px, h + 2 * py\n",
    "    new_img = Image.new(\"RGBA\", (new_w, new_h), color)\n",
    "    new_img.paste(im, (px, py))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef390b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.820401500Z",
     "start_time": "2025-11-12T16:40:27.765804300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_char_image(token: str) -> Image.Image:\n",
    "    p = random.choice(ASSETS[token])\n",
    "    return load_rgba(p)\n",
    "\n",
    "def random_rotation(im: Image.Image, max_deg=0):\n",
    "    ang = random.uniform(-max_deg, max_deg)\n",
    "    return im.rotate(ang, resample=Image.BICUBIC, expand=True)\n",
    "\n",
    "def random_perspective(im: Image.Image, max_warp=0.12):\n",
    "    # Simple affine approximation using PIL's transform with QUAD mapping\n",
    "    w, h = im.size\n",
    "    dx = int(w*max_warp)\n",
    "    dy = int(h*max_warp)\n",
    "    src = (0,0, w,0, w,h, 0,h)\n",
    "    dst = (\n",
    "        random.randint(-dx, dx), random.randint(-dy, dy),\n",
    "        w+random.randint(-dx, dx), random.randint(-dy, dy),\n",
    "        w+random.randint(-dx, dx), h+random.randint(-dy, dy),\n",
    "        random.randint(-dx, dx),   h+random.randint(-dy, dy)\n",
    "    )\n",
    "    return im.transform((w,h), Image.QUAD, dst, resample=Image.BICUBIC)\n",
    "\n",
    "def random_blur(im: Image.Image, p=0.35):\n",
    "    if random.random()<p:\n",
    "        return im.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3,1.2)))\n",
    "    return im\n",
    "\n",
    "def add_occlusions(im: Image.Image, p_line=0.6, p_dots=0.5):\n",
    "    im = im.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    w,h = im.size\n",
    "    if random.random()<p_line:\n",
    "        for _ in range(random.randint(1,3)):\n",
    "            x1,y1 = random.randint(0,w),random.randint(0,h)\n",
    "            x2,y2 = random.randint(0,w),random.randint(0,h)\n",
    "            col = (random.randint(0,180),random.randint(0,180),random.randint(0,180),random.randint(90,170))\n",
    "            draw.line((x1,y1,x2,y2), fill=col, width=random.randint(1,2))\n",
    "    if random.random()<p_dots:\n",
    "        for _ in range(random.randint(20,60)):\n",
    "            x,y = random.randint(0,w),random.randint(0,h)\n",
    "            r = random.randint(0,1)\n",
    "            col = (random.randint(0,180),random.randint(0,180),random.randint(0,180),random.randint(90,170))\n",
    "            draw.ellipse((x-r,y-r,x+r,y+r), fill=col)\n",
    "    return im\n",
    "\n",
    "def compose_line(images: List[Image.Image], spacing=(6,20), vjitter=(-3,6), bg=(255,255,255)):\n",
    "    heights = [im.size[1] for im in images]\n",
    "    max_h = max(heights)+8\n",
    "    gaps = [random.randint(*spacing) for _ in range(len(images)-1)]\n",
    "    total_w = sum(im.size[0] for im in images)+sum(gaps)\n",
    "    canvas = Image.new(\"RGBA\",(total_w,max_h), bg+(255,))\n",
    "    x=0\n",
    "    for i,im in enumerate(images):\n",
    "        y = random.randint(*vjitter)\n",
    "        canvas.paste(im,(x,max(0,y)),im)\n",
    "        x += im.size[0]\n",
    "        if i<len(images)-1: x += gaps[i]\n",
    "    return canvas\n",
    "\n",
    "def classical_generate(text: str, size=(48,48), augment=False, bg=(255,255,255)):\n",
    "    ims=[]\n",
    "    for ch in text:\n",
    "        token = pick_token_for_char(ch)\n",
    "        im = sample_char_image(token).resize(size, Image.BICUBIC)\n",
    "        im = add_padding(im, 1, 0.05)\n",
    "        if augment:\n",
    "            im = random_rotation(im, 18)\n",
    "            im = random_perspective(im, 0.04)\n",
    "            im = random_blur(im, 0.25)\n",
    "        ims.append(im)\n",
    "    out = compose_line(ims, spacing=(6,20), vjitter=(-3,6), bg=bg)\n",
    "    out = add_occlusions(out, 0.6, 0.5)\n",
    "    out = random_blur(out, 0.25).convert(\"RGB\")\n",
    "    return out\n",
    "\n",
    "# # Demo (requires data):\n",
    "# img = classical_generate(\"Ab9X2\")\n",
    "# show_image(img, \"Classical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906501c",
   "metadata": {},
   "source": [
    "## 5. Conditional DCGAN (per-character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0aac7a",
   "metadata": {},
   "source": [
    "### 5.0 Important: Data Normalization\n",
    "\n",
    "**Critical:** The `CharSpriteDataset` must normalize images to `[-1, 1]` to match the Generator's `tanh` output.\n",
    "\n",
    "The dataset already does this with `T.Normalize([0.5], [0.5])`, which transforms `[0,1] ‚Üí [-1,1]`.\n",
    "\n",
    "**Verify your data:**\n",
    "- Dataset outputs: `shape=[B,1,H,W]`, `range=[-1,1]`\n",
    "- Generator outputs: `shape=[B,1,H,W]`, `range=[-1,1]` (via tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0274324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.831920700Z",
     "start_time": "2025-11-12T16:40:27.784367200Z"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Dataset for training cGAN\n",
    "# =====================================================\n",
    "class CharSpriteDataset(Dataset):\n",
    "    def __init__(self, assets: Dict[str, List[Path]], token_to_idx: dict, \n",
    "                 image_size=48, grayscale=True, normalize=True):\n",
    "        self.samples = []\n",
    "        for token, paths in assets.items():\n",
    "            idx = token_to_idx[token]\n",
    "            for p in paths:\n",
    "                self.samples.append((p, idx))\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.grayscale = grayscale\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Build transform pipeline\n",
    "        transform_list = [\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor()  # This converts PIL [0, 255] -> torch [0.0, 1.0]\n",
    "        ]\n",
    "        \n",
    "        if normalize:\n",
    "            # Normalize from [0, 1] to [-1, 1]\n",
    "            # Formula: output = (input - mean) / std\n",
    "            # For [0,1] -> [-1,1]: mean=0.5, std=0.5\n",
    "            # This works for grayscale (1 channel)\n",
    "            transform_list.append(T.Normalize(mean=[0.5], std=[0.5]))\n",
    "        \n",
    "        self.transform = T.Compose(transform_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        # Load image and ensure it's grayscale or RGB as specified\n",
    "        img = Image.open(path).convert('L' if self.grayscale else 'RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e1bbd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.832920700Z",
     "start_time": "2025-11-12T16:40:27.803845600Z"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# MODEL BLOCK: Conditional WGAN-GP with ResNet blocks\n",
    "# =====================================================\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as SN\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.f = SN(nn.Conv2d(ch, ch//8, 1))\n",
    "        self.g = SN(nn.Conv2d(ch, ch//8, 1))\n",
    "        self.h = SN(nn.Conv2d(ch, ch, 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        f = self.f(x).view(B, -1, H*W)\n",
    "        g = self.g(x).view(B, -1, H*W)\n",
    "        attn = F.softmax(torch.bmm(f.permute(0,2,1), g), dim=-1)\n",
    "        h = self.h(x).view(B, -1, H*W)\n",
    "        out = torch.bmm(h, attn).view(B,C,H,W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class ResBlockG(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.c1 = SN(nn.Conv2d(in_ch, out_ch, 3, 1, 1))\n",
    "        self.c2 = SN(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        self.bn1, self.bn2 = nn.BatchNorm2d(in_ch), nn.BatchNorm2d(out_ch)\n",
    "        self.skip = SN(nn.Conv2d(in_ch, out_ch, 1, 1, 0)) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        skip = self.skip(self.up(x))\n",
    "        y = self.up(x)\n",
    "        y = F.relu(self.bn1(y))\n",
    "        y = self.c1(y)\n",
    "        y = F.relu(self.bn2(y))\n",
    "        y = self.c2(y)\n",
    "        return y + skip\n",
    "\n",
    "class ResBlockD(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, down=True):\n",
    "        super().__init__()\n",
    "        self.down = down\n",
    "        self.c1, self.c2 = SN(nn.Conv2d(in_ch, out_ch, 3, 1, 1)), SN(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        self.skip = SN(nn.Conv2d(in_ch, out_ch, 1, 1, 0)) if in_ch != out_ch else nn.Identity()\n",
    "        self.avg = nn.AvgPool2d(2) if down else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        skip = self.avg(self.skip(x))\n",
    "        y = F.leaky_relu(self.c1(x), 0.2, inplace=True)\n",
    "        y = F.leaky_relu(self.c2(y), 0.2, inplace=True)\n",
    "        y = self.avg(y)\n",
    "        return y + skip\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, nclass, base=64, im_size=48, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(nclass, embed_dim)\n",
    "        self.fc = nn.Linear(z_dim + embed_dim, base*8*4*4)\n",
    "        self.b1 = ResBlockG(base*8, base*4)\n",
    "        self.b2 = ResBlockG(base*4, base*2)\n",
    "        self.b3 = ResBlockG(base*2, base)\n",
    "        self.sa  = SelfAttention(base)\n",
    "        self.b4 = ResBlockG(base, base)\n",
    "        self.to_rgb = SN(nn.Conv2d(base, 1, 3, 1, 1))\n",
    "        self.im_size = im_size\n",
    "    def forward(self, z, y):\n",
    "        yemb = self.embed(y)\n",
    "        x = torch.cat([z, yemb], 1)\n",
    "        x = self.fc(x).view(x.size(0), -1, 4, 4)\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x)\n",
    "        x = self.sa(x); x = self.b4(x)\n",
    "        x = torch.tanh(self.to_rgb(F.relu(x)))\n",
    "        if x.shape[-1] != self.im_size:\n",
    "            x = F.interpolate(x, size=(self.im_size, self.im_size), mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nclass, base=64):\n",
    "        super().__init__()\n",
    "        self.b1 = ResBlockD(1, base)\n",
    "        self.b2 = ResBlockD(base, base*2)\n",
    "        self.sa  = SelfAttention(base*2)\n",
    "        self.b3 = ResBlockD(base*2, base*4)\n",
    "        self.b4 = ResBlockD(base*4, base*8)\n",
    "        \n",
    "        # Use global average pooling instead of sum to prevent explosion\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Simpler projection-based conditioning\n",
    "        self.lin = SN(nn.Linear(base*8, 1))\n",
    "        self.embed = nn.Embedding(nclass, base*8)\n",
    "        \n",
    "        # Initialize embedding with small values\n",
    "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, x, y, return_feat=False):\n",
    "        h = self.b1(x)\n",
    "        h = self.b2(h)\n",
    "        h = self.sa(h)\n",
    "        h = self.b3(h)\n",
    "        h = self.b4(h)                              # [B, base*8, H, W]\n",
    "        \n",
    "        # Global average pooling - much more stable than sum\n",
    "        feat = self.pool(h).squeeze(-1).squeeze(-1) # [B, base*8]\n",
    "        \n",
    "        # Main critic score\n",
    "        out = self.lin(feat).squeeze(1)             # [B]\n",
    "        \n",
    "        # Class conditioning via inner product (scaled)\n",
    "        emb = self.embed(y)                         # [B, base*8]\n",
    "        proj = torch.sum(emb * feat, dim=1) * 0.1  # [B] - scale down projection\n",
    "        \n",
    "        if return_feat:\n",
    "            return out + proj, feat\n",
    "        return out + proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa90c53",
   "metadata": {},
   "source": [
    "### 5.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b4fbc67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T17:20:05.552098300Z",
     "start_time": "2025-11-12T17:20:05.547040100Z"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TRAINING BLOCK: WGAN-GP + TTUR + Feature Matching\n",
    "# =====================================================\n",
    "import torchvision.utils as vutils\n",
    "from pathlib import Path\n",
    "\n",
    "def gradient_penalty(D, real, fake, y, device):\n",
    "    Œ± = torch.rand(real.size(0),1,1,1,device=device)\n",
    "    inter = (Œ±*real + (1-Œ±)*fake).requires_grad_(True)\n",
    "    score = D(inter, y)\n",
    "    grad = torch.autograd.grad(outputs=score, inputs=inter,\n",
    "                               grad_outputs=torch.ones_like(score),\n",
    "                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gp = ((grad.view(grad.size(0), -1).norm(2, dim=1)-1)**2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_vis(G, z_fixed, y_fixed, epoch, save_dir):\n",
    "    G.eval()\n",
    "    img = G(z_fixed, y_fixed)\n",
    "    grid = vutils.make_grid(img, nrow=8, normalize=True, value_range=(-1,1))\n",
    "    vutils.save_image(grid, f\"{save_dir}/vis_{epoch:03d}.png\")\n",
    "    G.train()\n",
    "\n",
    "def train_wgangp(dl, nclass, z_dim=96, epochs=20, lr_G=1e-5, lr_D=5e-5, base=32, gp_lambda=10, n_critic=5, use_feature_matching=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    G, D = Generator(z_dim, nclass, base).to(device), Discriminator(nclass, base).to(device)\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=lr_G, betas=(0.0,0.9))\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=lr_D, betas=(0.0,0.9))\n",
    "    fixed_z = torch.randn(64, z_dim, device=device)\n",
    "    fixed_y = torch.tensor([i % nclass for i in range(64)], device=device)\n",
    "    save_dir = Path(\"wgan_training\"); save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üöÄ Starting training: {len(dl)} batches/epoch, {nclass} classes\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        epoch_D_loss = 0\n",
    "        epoch_G_loss = 0\n",
    "        epoch_real_score = 0\n",
    "        epoch_fake_score = 0\n",
    "        epoch_gp = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for real, y in dl:\n",
    "            real, y = real.to(device), y.to(device)\n",
    "            b = real.size(0)\n",
    "            n_batches += 1\n",
    "            \n",
    "            # --- Train D multiple times ---\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(b, z_dim, device=device)\n",
    "                fake = G(z, y).detach()\n",
    "                D.zero_grad(set_to_none=True)\n",
    "                real_score = D(real, y)\n",
    "                fake_score = D(fake, y)\n",
    "                gp = gradient_penalty(D, real, fake, y, device) * gp_lambda\n",
    "                loss_D = fake_score.mean() - real_score.mean() + gp\n",
    "                loss_D.backward()\n",
    "                \n",
    "                # More aggressive gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(D.parameters(), 1.0)\n",
    "                optD.step()\n",
    "                \n",
    "            # Accumulate stats\n",
    "            epoch_real_score += real_score.mean().item()\n",
    "            epoch_fake_score += fake_score.mean().item()\n",
    "            epoch_gp += gp.item()\n",
    "            epoch_D_loss += loss_D.item()\n",
    "            \n",
    "            # Debug output (throttled) - check for explosion\n",
    "            if n_batches % 50 == 0:\n",
    "                real_m = real_score.mean().item()\n",
    "                fake_m = fake_score.mean().item()\n",
    "                gp_m = gp.item()\n",
    "                print(f\"  [Batch {n_batches}] real={real_m:.2f} fake={fake_m:.2f} gp={gp_m:.3f} lossD={loss_D.item():.3f}\")\n",
    "                \n",
    "                # Early warning if scores explode\n",
    "                if abs(real_m) > 100 or abs(fake_m) > 100:\n",
    "                    print(f\"  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\")\n",
    "            \n",
    "            # --- Train G ---\n",
    "            z = torch.randn(b, z_dim, device=device)\n",
    "            fake = G(z, y)\n",
    "            \n",
    "            if use_feature_matching:\n",
    "                fake_score, fake_feat = D(fake, y, True)\n",
    "                with torch.no_grad(): _, real_feat = D(real, y, True)\n",
    "                fm_loss = F.l1_loss(fake_feat, real_feat) * 5.0  # Reduced from 10.0\n",
    "                loss_G = -fake_score.mean() + fm_loss\n",
    "            else:\n",
    "                fake_score = D(fake, y)\n",
    "                loss_G = -fake_score.mean()\n",
    "            \n",
    "            G.zero_grad(set_to_none=True)\n",
    "            loss_G.backward()\n",
    "            \n",
    "            # More aggressive gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(G.parameters(), 1.0)\n",
    "            optG.step()\n",
    "            \n",
    "            epoch_G_loss += loss_G.item()\n",
    "            \n",
    "        # Average stats for the epoch\n",
    "        avg_D_loss = epoch_D_loss / n_batches\n",
    "        avg_G_loss = epoch_G_loss / n_batches\n",
    "        avg_real = epoch_real_score / n_batches\n",
    "        avg_fake = epoch_fake_score / n_batches\n",
    "        avg_gp = epoch_gp / n_batches\n",
    "        \n",
    "        save_vis(G, fixed_z, fixed_y, ep, save_dir)\n",
    "        print(f\"Epoch {ep:03d} | D: {avg_D_loss:.3f} | G: {avg_G_loss:.3f} | real: {avg_real:.2f} | fake: {avg_fake:.2f} | gp: {avg_gp:.3f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(G.state_dict(), f\"{save_dir}/G_ep{ep:03d}.pt\")\n",
    "        \n",
    "        # Stop if completely diverged\n",
    "        if abs(avg_real) > 1000 or abs(avg_fake) > 1000:\n",
    "            print(\"‚ùå Training diverged - scores exploded. Stopping early.\")\n",
    "            break\n",
    "            \n",
    "    print(\"‚úÖ Training complete.\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b9d8c",
   "metadata": {},
   "source": [
    "### 5.2 Inference & Unified API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f143010a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T17:20:22.316674700Z",
     "start_time": "2025-11-12T17:20:22.310145300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_generator(ckpt_path: str):\n",
    "    # --- load the upgraded WGAN-GP generator ---\n",
    "    G = Generator(z_dim=NZ, nclass=NCLASS, base=32, im_size=IMG_SIZE).to(DEVICE)\n",
    "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    G.load_state_dict(state)\n",
    "    G.eval()\n",
    "    return G\n",
    "\n",
    "\n",
    "def generate_char_with_cgan(G, token: str, size=(48,48)) -> Image.Image:\n",
    "    y = torch.tensor([TOKEN_TO_IDX[token]], device=DEVICE, dtype=torch.long)\n",
    "    z = torch.randn(1, NZ, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = G(z, y)  # [-1,1]\n",
    "    arr = out.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    arr = (arr*0.5 + 0.5) * 255.0\n",
    "    arr = arr.clip(0,255).astype(np.uint8)\n",
    "    pil = Image.fromarray(arr, mode=\"L\").convert(\"RGBA\")\n",
    "    # crude alpha\n",
    "    alpha = pil.convert(\"L\").point(lambda v: 255 if v>15 else 0)\n",
    "    pil.putalpha(alpha)\n",
    "    if size: pil = pil.resize(size, Image.BICUBIC)\n",
    "    return pil\n",
    "\n",
    "def cgan_generate(text: str, G, size=(48,48), augment=True, bg=(255,255,255)):\n",
    "    ims=[]\n",
    "    for ch in text:\n",
    "        token = pick_token_for_char(ch)\n",
    "        im = generate_char_with_cgan(G, token, size=size)\n",
    "        if augment:\n",
    "            im = random_rotation(im, 18)\n",
    "            im = random_perspective(im, 0.12)\n",
    "            im = random_blur(im, 0.35)\n",
    "        ims.append(im)\n",
    "    out = compose_line(ims, spacing=(6,20), vjitter=(-3,6), bg=bg)\n",
    "    out = add_occlusions(out, 0.6, 0.5)\n",
    "    out = random_blur(out, 0.25).convert(\"RGB\")\n",
    "    return out\n",
    "\n",
    "class CaptchaGenerator:\n",
    "    def __init__(self, mode=\"classical\", g_ckpt: Optional[str]=None, bg=(255,255,255)):\n",
    "        self.mode = mode\n",
    "        self.bg = bg\n",
    "        self.G = None\n",
    "        if mode==\"cgan\":\n",
    "            assert g_ckpt and Path(g_ckpt).exists(), \"Provide a valid generator checkpoint\"\n",
    "            self.G = load_generator(g_ckpt)\n",
    "    def generate(self, text: str, size=(48,48), augment=True, save_path: Optional[str]=None) -> Image.Image:\n",
    "        if self.mode==\"classical\":\n",
    "            out = classical_generate(text, size=size, augment=augment, bg=self.bg)\n",
    "        else:\n",
    "            out = cgan_generate(text, self.G, size=size, augment=augment, bg=self.bg)\n",
    "        if save_path:\n",
    "            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            out.save(save_path)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39b4a1",
   "metadata": {},
   "source": [
    "## 6. Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c0991",
   "metadata": {},
   "source": [
    "### 5.3 Debug: Check Raw Image Data\n",
    "\n",
    "Run this cell to verify your raw image files and transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50fa09ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.885856Z",
     "start_time": "2025-11-12T16:40:27.868568700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing image: data\\chars_by_class\\digits\\0\\0024miih-0_char0.png\n",
      "Token: 0\n",
      "\n",
      "1. Raw PIL Image:\n",
      "   Mode: L, Size: (23, 32)\n",
      "   Array range: [219, 255]\n",
      "\n",
      "2. After ToTensor:\n",
      "   Shape: torch.Size([1, 32, 23]), Range: [0.859, 1.000]\n",
      "\n",
      "3. After Full Transform (Resize + ToTensor + Normalize):\n",
      "   Shape: torch.Size([1, 48, 48]), Range: [0.718, 1.000]\n",
      "\n",
      "‚ùå ERROR: Normalization failed! Images may have black backgrounds.\n",
      "   Your images might be mostly white (high pixel values).\n",
      "   For white background images, this is actually normal.\n",
      "   The issue: character pixels might be in a narrow range.\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what's happening with the images\n",
    "import numpy as np\n",
    "\n",
    "# Pick a sample image path\n",
    "sample_token = list(ASSETS.keys())[0]\n",
    "sample_path = ASSETS[sample_token][0]\n",
    "\n",
    "print(f\"Testing image: {sample_path}\")\n",
    "print(f\"Token: {sample_token}\")\n",
    "\n",
    "# Load raw PIL image\n",
    "raw_pil = Image.open(sample_path).convert('L')\n",
    "print(f\"\\n1. Raw PIL Image:\")\n",
    "print(f\"   Mode: {raw_pil.mode}, Size: {raw_pil.size}\")\n",
    "raw_array = np.array(raw_pil)\n",
    "print(f\"   Array range: [{raw_array.min()}, {raw_array.max()}]\")\n",
    "\n",
    "# Apply ToTensor only\n",
    "to_tensor = T.ToTensor()\n",
    "tensor_only = to_tensor(raw_pil)\n",
    "print(f\"\\n2. After ToTensor:\")\n",
    "print(f\"   Shape: {tensor_only.shape}, Range: [{tensor_only.min():.3f}, {tensor_only.max():.3f}]\")\n",
    "\n",
    "# Apply full transform\n",
    "full_transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "transformed = full_transform(raw_pil)\n",
    "print(f\"\\n3. After Full Transform (Resize + ToTensor + Normalize):\")\n",
    "print(f\"   Shape: {transformed.shape}, Range: [{transformed.min():.3f}, {transformed.max():.3f}]\")\n",
    "\n",
    "# Expected: should be close to [-1, 1]\n",
    "if transformed.min() > -0.5 or transformed.max() < 0.5:\n",
    "    print(\"\\n‚ùå ERROR: Normalization failed! Images may have black backgrounds.\")\n",
    "    print(\"   Your images might be mostly white (high pixel values).\")\n",
    "    print(\"   For white background images, this is actually normal.\")\n",
    "    print(\"   The issue: character pixels might be in a narrow range.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Normalization looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc592f",
   "metadata": {},
   "source": [
    "### 5.4 Fix: Invert Images (if needed)\n",
    "\n",
    "**If your debug shows images are in [0.72, 1.00] range**, your images have **white backgrounds with dark text**. This causes a narrow normalized range and training instability.\n",
    "\n",
    "**Solution:** Invert the grayscale images so black becomes white and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef714b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:40:27.956546500Z",
     "start_time": "2025-11-12T16:40:27.882857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FIXED dataset with inversion...\n",
      "Sample 0: shape=torch.Size([1, 48, 48]), range=[-1.000, -0.718]\n",
      "Sample 1: shape=torch.Size([1, 48, 48]), range=[-1.000, -0.067]\n",
      "Sample 2: shape=torch.Size([1, 48, 48]), range=[-1.000, -0.357]\n",
      "\n",
      "‚úì If range is now close to [-1, 1], use CharSpriteDatasetV2 instead!\n"
     ]
    }
   ],
   "source": [
    "# Updated Dataset with optional inversion\n",
    "class CharSpriteDatasetV2(Dataset):\n",
    "    def __init__(self, assets: Dict[str, List[Path]], token_to_idx: dict, \n",
    "                 image_size=48, grayscale=True, normalize=True, invert=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            invert: If True, inverts grayscale images (255-x). \n",
    "                    Use this if your images have white backgrounds!\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        for token, paths in assets.items():\n",
    "            idx = token_to_idx[token]\n",
    "            for p in paths:\n",
    "                self.samples.append((p, idx))\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.grayscale = grayscale\n",
    "        self.normalize = normalize\n",
    "        self.invert = invert\n",
    "        \n",
    "        # Build transform pipeline - NO normalization here\n",
    "        self.base_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor()  # [0, 255] -> [0.0, 1.0]\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('L' if self.grayscale else 'RGB')\n",
    "        \n",
    "        # Apply base transform\n",
    "        img = self.base_transform(img)  # Now in [0, 1]\n",
    "        \n",
    "        # Invert if needed (for white background images)\n",
    "        if self.invert and self.grayscale:\n",
    "            img = 1.0 - img  # Flip: white->black, black->white\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        if self.normalize:\n",
    "            img = (img - 0.5) / 0.5  # [0,1] -> [-1,1]\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Test the fixed dataset\n",
    "print(\"Testing FIXED dataset with inversion...\")\n",
    "ds_fixed = CharSpriteDatasetV2(ASSETS, TOKEN_TO_IDX, image_size=IMG_SIZE, \n",
    "                                grayscale=True, normalize=True, invert=True)\n",
    "\n",
    "for i in range(3):\n",
    "    img, label = ds_fixed[i]\n",
    "    print(f\"Sample {i}: shape={img.shape}, range=[{img.min():.3f}, {img.max():.3f}]\")\n",
    "\n",
    "print(\"\\n‚úì If range is now close to [-1, 1], use CharSpriteDatasetV2 instead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b03207ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 50] real=-40.92 fake=-48.60 gp=0.643 lossD=-7.042\n",
      "  [Batch 100] real=-3.81 fake=-11.32 gp=0.541 lossD=-6.974\n",
      "  [Batch 150] real=-85.34 fake=-93.57 gp=0.593 lossD=-7.638\n",
      "  [Batch 200] real=36.25 fake=27.86 gp=1.287 lossD=-7.107\n",
      "  [Batch 250] real=-21.85 fake=-29.31 gp=1.176 lossD=-6.281\n",
      "  [Batch 300] real=69.70 fake=63.14 gp=0.459 lossD=-6.104\n",
      "  [Batch 350] real=123.85 fake=118.28 gp=0.800 lossD=-4.773\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "Epoch 001 | D: -7.345 | G: 19.025 | real: -7.76 | fake: -16.60 | gp: 1.502\n",
      "  [Batch 50] real=116.41 fake=109.63 gp=1.880 lossD=-4.897\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 100] real=122.52 fake=115.61 gp=1.984 lossD=-4.924\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 150] real=186.58 fake=180.56 gp=0.369 lossD=-5.651\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 200] real=254.31 fake=247.25 gp=0.956 lossD=-6.104\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 250] real=334.07 fake=326.09 gp=2.488 lossD=-5.483\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 300] real=366.65 fake=360.28 gp=0.561 lossD=-5.806\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 350] real=415.28 fake=410.34 gp=0.548 lossD=-4.391\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "Epoch 002 | D: -5.821 | G: -241.965 | real: 243.57 | fake: 236.77 | gp: 0.977\n",
      "  [Batch 50] real=505.09 fake=497.34 gp=2.589 lossD=-5.169\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 100] real=821.47 fake=813.94 gp=0.494 lossD=-7.040\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 150] real=711.70 fake=703.89 gp=0.742 lossD=-7.062\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 200] real=1049.95 fake=1043.06 gp=0.482 lossD=-6.412\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 250] real=1029.94 fake=1022.99 gp=0.618 lossD=-6.335\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 300] real=1378.82 fake=1371.20 gp=0.513 lossD=-7.113\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 350] real=1089.24 fake=1083.83 gp=0.585 lossD=-4.820\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "Epoch 003 | D: -5.248 | G: -861.539 | real: 866.14 | fake: 860.09 | gp: 0.802\n",
      "  [Batch 50] real=1370.77 fake=1366.01 gp=0.587 lossD=-4.178\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 100] real=1447.24 fake=1442.25 gp=0.699 lossD=-4.289\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 150] real=1370.89 fake=1367.49 gp=0.818 lossD=-2.578\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 200] real=1682.93 fake=1675.65 gp=0.305 lossD=-6.971\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 250] real=1528.46 fake=1523.72 gp=0.713 lossD=-4.032\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 300] real=1810.51 fake=1804.77 gp=0.315 lossD=-5.424\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "  [Batch 350] real=1655.30 fake=1650.27 gp=0.292 lossD=-4.739\n",
      "  ‚ö†Ô∏è  WARNING: Discriminator scores exploding! Consider reducing learning rate or model capacity.\n",
      "Epoch 004 | D: -4.632 | G: -1515.605 | real: 1520.27 | fake: 1515.10 | gp: 0.541\n",
      "‚ùå Training diverged - scores exploded. Stopping early.\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "ds = CharSpriteDataset(ASSETS, TOKEN_TO_IDX, image_size=IMG_SIZE, grayscale=True, normalize=True)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# CRITICAL: Verify data normalization\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFYING DATASET NORMALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check multiple samples to see the actual range\n",
    "sample_ranges = []\n",
    "for i in range(min(10, len(ds))):\n",
    "    img, label = ds[i]\n",
    "    sample_ranges.append((img.min().item(), img.max().item()))\n",
    "    if i < 3:\n",
    "        print(f\"Sample {i}: shape={img.shape}, range=[{img.min():.3f}, {img.max():.3f}]\")\n",
    "\n",
    "avg_min = np.mean([r[0] for r in sample_ranges])\n",
    "avg_max = np.mean([r[1] for r in sample_ranges])\n",
    "print(f\"\\nAverage range across {len(sample_ranges)} samples: [{avg_min:.3f}, {avg_max:.3f}]\")\n",
    "\n",
    "# Check if normalization is correct\n",
    "if avg_min > -0.5 or avg_max < 0.5:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Data NOT in [-1, 1] range!\")\n",
    "    print(\"   Your images likely have white backgrounds (high pixel values).\")\n",
    "    print(\"   This will cause training instability.\")\n",
    "    print(\"\\n   RECOMMENDED: Run the debug cell above (section 5.3) first!\")\n",
    "    print(\"   Then either:\")\n",
    "    print(\"   1. Invert your images (white->black, black->white), OR\")\n",
    "    print(\"   2. Use much smaller learning rates below\")\n",
    "    \n",
    "    # Emergency fallback: very conservative training\n",
    "    print(\"\\n   Using EMERGENCY conservative settings...\")\n",
    "    lr_g, lr_d, gp_lam = 1e-5, 5e-5, 20\n",
    "else:\n",
    "    print(\"\\n‚úì Data normalization looks correct!\")\n",
    "    lr_g, lr_d, gp_lam = 5e-5, 2e-4, 15\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the WGAN-GP model\n",
    "G = train_wgangp(\n",
    "    dl, \n",
    "    nclass=NCLASS, \n",
    "    z_dim=NZ, \n",
    "    epochs=15, \n",
    "    base=32,                # REDUCED from 64 for stability\n",
    "    lr_G=lr_g,\n",
    "    lr_D=lr_d,\n",
    "    gp_lambda=gp_lam,\n",
    "    n_critic=5,\n",
    "    use_feature_matching=False\n",
    ")\n",
    "torch.save(G.state_dict(), \"G_wgangp.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703f81e",
   "metadata": {},
   "source": [
    "### 6.1 Training Tips & Expected Behavior\n",
    "\n",
    "**Healthy training signs:**\n",
    "- Scores in reasonable range: `real ~ -5 to +10`, `fake ~ -10 to +5`\n",
    "- `real_mean > fake_mean` by modest margin (e.g., real=2.5, fake=-1.8)\n",
    "- `gp ~ 0.5‚Äì3.0` (depends on Œª=15)\n",
    "- `loss_D` should be small, typically `-5 to +5`\n",
    "- `loss_G` negative and slowly increasing toward 0 (e.g., -8 ‚Üí -5 ‚Üí -3)\n",
    "\n",
    "**Red flags (training diverged):**\n",
    "- Scores exploding: `real > 100` or `fake > 100` ‚Üí **STOP IMMEDIATELY**\n",
    "- Wild swings: `¬±100` changes between batches\n",
    "- GP exploding: `gp > 50`\n",
    "\n",
    "**If scores still explode:**\n",
    "1. **Reduce model capacity**: Change `base=NGF` to `base=32` in training call\n",
    "2. **Lower learning rates further**: `lr_G=1e-5, lr_D=5e-5`\n",
    "3. **Increase batch size**: `BATCH_SIZE=256` (if memory allows) for more stable gradients\n",
    "4. **Check data**: Run the verification code above - must be `[-1, 1]`\n",
    "\n",
    "**Architecture changes made to prevent explosion:**\n",
    "- Changed `torch.sum()` ‚Üí `AdaptiveAvgPool2d()` in discriminator (critical fix)\n",
    "- Removed layer normalization that could cause instability\n",
    "- Scaled projection term down (√ó0.1)\n",
    "- More aggressive gradient clipping (1.0 instead of 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190445de",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-12T17:18:11.962069400Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_classical = CaptchaGenerator(mode=\"classical\")\n",
    "gen_gan = CaptchaGenerator(mode=\"cgan\", g_ckpt=\"G_wgangp.pt\")\n",
    "\n",
    "gen_str = \"x4GTsLI\"\n",
    "img1 = gen_classical.generate(gen_str, augment=False)\n",
    "img2 = gen_gan.generate(gen_str, augment=False)\n",
    "\n",
    "show_image(img1, \"Classical\")\n",
    "show_image(img2, \"WGAN-GP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6c3de",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Notes\n",
    "- If some classes are scarce, consider oversampling or weighted sampling during cGAN training.\n",
    "- Increase `IMG_SIZE` to 64 and adjust conv stack for more detail.\n",
    "- You can add elastic warps, shadows, curved baselines in the **whole-canvas** step.\n",
    "- For fully end-to-end sequence generators (string‚Üíimage), consider diffusion or transformer-based renderers later; \n",
    "  here we keep it robust and practical for your dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4243_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
