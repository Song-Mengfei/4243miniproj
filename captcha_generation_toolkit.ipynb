{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1eab1dd",
   "metadata": {},
   "source": [
    "\n",
    "# CAPTCHA Generator: Classical Compositor + Conditional DCGAN\n",
    "\n",
    "This notebook accepts a **string** and outputs a **CAPTCHA image** using two methods:\n",
    "1) **Classical**: sample segmented character assets + augment + compose.\n",
    "2) **cGAN**: train a conditional DCGAN to generate character sprites, then compose.\n",
    "\n",
    "**Expected data structure** (confirmed):\n",
    "```\n",
    "chars_by_class/\n",
    " ├── 0/\n",
    " ├── 1/\n",
    " ├── 2/\n",
    " ├── ...\n",
    " ├── a/\n",
    " │   ├── A_cap/\n",
    " │   └── a_small/\n",
    " ├── b/\n",
    " │   ├── B_cap/\n",
    " │   └── b_small/\n",
    " ├── j/        # no cap/small subfolders if visually identical\n",
    " └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb553c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05bc89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %pip install torch torchvision opencv-python pillow matplotlib tqdm\n",
    "\n",
    "import os, random, string\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFilter\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61966cb8",
   "metadata": {},
   "source": [
    "## 2. Scan Dataset & Build Token Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f25a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 tokens/classes.\n",
      "  0 -> 1301 images\n",
      "  1 -> 1333 images\n",
      "  2 -> 1237 images\n",
      "  3 -> 1335 images\n",
      "  4 -> 1300 images\n",
      "  5 -> 1262 images\n",
      "  6 -> 1297 images\n",
      "  7 -> 1253 images\n",
      "  8 -> 1271 images\n",
      "  9 -> 1292 images\n",
      "  A_cap -> 577 images\n",
      "  B_cap -> 659 images\n",
      "  C_cap -> 1175 images\n",
      "  D_cap -> 587 images\n",
      "  E_cap -> 537 images\n",
      "  F_cap -> 610 images\n",
      "  G_cap -> 593 images\n",
      "  H_cap -> 592 images\n",
      "  I_cap -> 863 images\n",
      "  L_cap -> 289 images\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"chars_by_class\")  # main root\n",
    "\n",
    "DIGITS_DIR = DATA_ROOT / \"digits\"\n",
    "LETTERS_DIR = DATA_ROOT / \"letters\"\n",
    "\n",
    "def scan_dataset(root: Path):\n",
    "    assets: Dict[str, List[Path]] = {}\n",
    "    token_meta: Dict[str, dict] = {}\n",
    "\n",
    "    # ----- Digits -----\n",
    "    for ddir in (DIGITS_DIR.iterdir() if DIGITS_DIR.exists() else []):\n",
    "        if ddir.is_dir():\n",
    "            d = ddir.name\n",
    "            paths = [p for p in ddir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            if paths:\n",
    "                assets[d] = paths\n",
    "                token_meta[d] = {\"kind\": \"digit\", \"char\": d, \"source\": str(ddir)}\n",
    "\n",
    "    # ----- Letters -----\n",
    "    for ldir in (LETTERS_DIR.iterdir() if LETTERS_DIR.exists() else []):\n",
    "        if not ldir.is_dir():\n",
    "            continue\n",
    "        ch = ldir.name\n",
    "        cap_dir = ldir / f\"{ch.upper()}_cap\"\n",
    "        small_dir = ldir / f\"{ch}_small\"\n",
    "        files_direct = [p for p in ldir.glob(\"*\") if p.is_file() and p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "\n",
    "        if cap_dir.is_dir() and small_dir.is_dir():\n",
    "            cap_paths = [p for p in cap_dir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            sm_paths  = [p for p in small_dir.rglob(\"*\") if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\"}]\n",
    "            if cap_paths:\n",
    "                token = f\"{ch.upper()}_cap\"\n",
    "                assets[token] = cap_paths\n",
    "                token_meta[token] = {\"kind\":\"letter_case\",\"char\":ch,\"case\":\"cap\",\"source\":str(cap_dir)}\n",
    "            if sm_paths:\n",
    "                token = f\"{ch}_small\"\n",
    "                assets[token] = sm_paths\n",
    "                token_meta[token] = {\"kind\":\"letter_case\",\"char\":ch,\"case\":\"small\",\"source\":str(small_dir)}\n",
    "        else:\n",
    "            if files_direct:\n",
    "                token = ch\n",
    "                assets[token] = files_direct\n",
    "                token_meta[token] = {\"kind\":\"letter_single\",\"char\":ch,\"source\":str(ldir)}\n",
    "\n",
    "    # ----- Mapping -----\n",
    "    idx_to_token = sorted(assets.keys())\n",
    "    token_to_idx = {t: i for i, t in enumerate(idx_to_token)}\n",
    "\n",
    "    print(f\"Found {len(idx_to_token)} tokens/classes.\")\n",
    "    for t in idx_to_token[:20]:\n",
    "        print(\" \", t, \"->\", len(assets[t]), \"images\")\n",
    "    if len(idx_to_token) > 20:\n",
    "        print(\" ...\")\n",
    "    return assets, token_meta, token_to_idx, idx_to_token\n",
    "\n",
    "ASSETS, TOKEN_META, TOKEN_TO_IDX, IDX_TO_TOKEN = scan_dataset(DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e1b8d",
   "metadata": {},
   "source": [
    "## 3. Utils: Image I/O & Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa6f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_rgba(path: Path) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "def show_image(im: Image.Image, title=None, size=None):\n",
    "    plt.figure()\n",
    "    if size: im = im.resize(size, Image.NEAREST)\n",
    "    plt.imshow(im)\n",
    "    if title: plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def pick_token_for_char(ch: str) -> str:\n",
    "    if ch.isdigit():\n",
    "        if ch in ASSETS: return ch\n",
    "        raise KeyError(f\"No assets for digit {ch}\")\n",
    "    if ch.isalpha():\n",
    "        l = ch.lower()\n",
    "        cap = f\"{l.upper()}_cap\"\n",
    "        small = f\"{l}_small\"\n",
    "        if ch.isupper():\n",
    "            if cap in ASSETS: return cap\n",
    "            if l in ASSETS: return l\n",
    "            if small in ASSETS: return small\n",
    "        else:\n",
    "            if small in ASSETS: return small\n",
    "            if l in ASSETS: return l\n",
    "            if cap in ASSETS: return cap\n",
    "    raise KeyError(f\"No asset token for '{ch}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77377fbf",
   "metadata": {},
   "source": [
    "## 4. Classical Compositional Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef390b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_char_image(token: str) -> Image.Image:\n",
    "    p = random.choice(ASSETS[token])\n",
    "    return load_rgba(p)\n",
    "\n",
    "def random_rotation(im: Image.Image, max_deg=18):\n",
    "    ang = random.uniform(-max_deg, max_deg)\n",
    "    return im.rotate(ang, resample=Image.BICUBIC, expand=True)\n",
    "\n",
    "def random_perspective(im: Image.Image, max_warp=0.12):\n",
    "    # Simple affine approximation using PIL's transform with QUAD mapping\n",
    "    w, h = im.size\n",
    "    dx = int(w*max_warp)\n",
    "    dy = int(h*max_warp)\n",
    "    src = (0,0, w,0, w,h, 0,h)\n",
    "    dst = (\n",
    "        random.randint(-dx, dx), random.randint(-dy, dy),\n",
    "        w+random.randint(-dx, dx), random.randint(-dy, dy),\n",
    "        w+random.randint(-dx, dx), h+random.randint(-dy, dy),\n",
    "        random.randint(-dx, dx),   h+random.randint(-dy, dy)\n",
    "    )\n",
    "    return im.transform((w,h), Image.QUAD, dst, resample=Image.BICUBIC)\n",
    "\n",
    "def random_blur(im: Image.Image, p=0.35):\n",
    "    if random.random()<p:\n",
    "        return im.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3,1.2)))\n",
    "    return im\n",
    "\n",
    "def add_occlusions(im: Image.Image, p_line=0.6, p_dots=0.5):\n",
    "    im = im.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    w,h = im.size\n",
    "    if random.random()<p_line:\n",
    "        for _ in range(random.randint(1,3)):\n",
    "            x1,y1 = random.randint(0,w),random.randint(0,h)\n",
    "            x2,y2 = random.randint(0,w),random.randint(0,h)\n",
    "            col = (random.randint(0,180),random.randint(0,180),random.randint(0,180),random.randint(90,170))\n",
    "            draw.line((x1,y1,x2,y2), fill=col, width=random.randint(1,2))\n",
    "    if random.random()<p_dots:\n",
    "        for _ in range(random.randint(20,60)):\n",
    "            x,y = random.randint(0,w),random.randint(0,h)\n",
    "            r = random.randint(0,1)\n",
    "            col = (random.randint(0,180),random.randint(0,180),random.randint(0,180),random.randint(90,170))\n",
    "            draw.ellipse((x-r,y-r,x+r,y+r), fill=col)\n",
    "    return im\n",
    "\n",
    "def compose_line(images: List[Image.Image], spacing=(6,20), vjitter=(-3,6), bg=(255,255,255)):\n",
    "    heights = [im.size[1] for im in images]\n",
    "    max_h = max(heights)+8\n",
    "    gaps = [random.randint(*spacing) for _ in range(len(images)-1)]\n",
    "    total_w = sum(im.size[0] for im in images)+sum(gaps)\n",
    "    canvas = Image.new(\"RGBA\",(total_w,max_h), bg+(255,))\n",
    "    x=0\n",
    "    for i,im in enumerate(images):\n",
    "        y = random.randint(*vjitter)\n",
    "        canvas.paste(im,(x,max(0,y)),im)\n",
    "        x += im.size[0]\n",
    "        if i<len(images)-1: x += gaps[i]\n",
    "    return canvas\n",
    "\n",
    "def classical_generate(text: str, size=(48,48), augment=True, bg=(255,255,255)):\n",
    "    ims=[]\n",
    "    for ch in text:\n",
    "        token = pick_token_for_char(ch)\n",
    "        im = sample_char_image(token).resize(size, Image.BICUBIC)\n",
    "        if augment:\n",
    "            im = random_rotation(im, 18)\n",
    "            im = random_perspective(im, 0.12)\n",
    "            im = random_blur(im, 0.35)\n",
    "        ims.append(im)\n",
    "    out = compose_line(ims, spacing=(6,20), vjitter=(-3,6), bg=bg)\n",
    "    out = add_occlusions(out, 0.6, 0.5)\n",
    "    out = random_blur(out, 0.25).convert(\"RGB\")\n",
    "    return out\n",
    "\n",
    "# # Demo (requires data):\n",
    "# img = classical_generate(\"Ab9X2\")\n",
    "# show_image(img, \"Classical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906501c",
   "metadata": {},
   "source": [
    "## 5. Conditional DCGAN (per-character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e1bbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharSpriteDataset(Dataset):\n",
    "    def __init__(self, assets: Dict[str, List[Path]], token_to_idx: Dict[str,int],\n",
    "                 image_size=48, grayscale=True, normalize=True):\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        self.image_size = image_size\n",
    "        self.grayscale = grayscale\n",
    "        self.normalize = normalize\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        for token, plist in assets.items():\n",
    "            y = token_to_idx[token]\n",
    "            for p in plist:\n",
    "                self.paths.append(p)\n",
    "                self.labels.append(y)\n",
    "\n",
    "        ts = [T.Resize((image_size,image_size), interpolation=T.InterpolationMode.BICUBIC)]\n",
    "        if grayscale: ts.append(T.Grayscale(num_output_channels=1))\n",
    "        ts.append(T.ToTensor())\n",
    "        if normalize: ts.append(T.Normalize([0.5], [0.5]))\n",
    "        self.tf = T.Compose(ts)\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        p = self.paths[i]; y = self.labels[i]\n",
    "        im = Image.open(p).convert(\"RGBA\")\n",
    "        bg = Image.new(\"RGBA\", im.size, (255,255,255,255))\n",
    "        bg.paste(im, (0,0), im)\n",
    "        x = self.tf(bg.convert(\"RGB\"))\n",
    "        return x, y\n",
    "\n",
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 128\n",
    "ds = CharSpriteDataset(ASSETS, TOKEN_TO_IDX, image_size=IMG_SIZE, grayscale=True, normalize=True)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "NZ = 96\n",
    "NCLASS = len(TOKEN_TO_IDX)\n",
    "EMBED_DIM = 64\n",
    "NGF = 64\n",
    "NDF = 64\n",
    "NC = 1\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(NCLASS, EMBED_DIM)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(NZ+EMBED_DIM, NGF*8*3*3),\n",
    "            nn.BatchNorm1d(NGF*8*3*3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NGF*8, NGF*4, 4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NGF*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(NGF*4, NGF*2, 4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NGF*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(NGF*2, NGF,   4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NGF),   nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(NGF, NC,      4,2,1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z, y):\n",
    "        emb = self.label_emb(y)\n",
    "        x = torch.cat([z, emb], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), NGF*8, 3, 3)\n",
    "        return self.net(x)\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(NCLASS, EMBED_DIM)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(NC, NDF, 4,2,1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(NDF, NDF*2, 4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NDF*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(NDF*2, NDF*4, 4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NDF*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(NDF*4, NDF*8, 4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(NDF*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.out = nn.Conv2d(NDF*8, 1, 3,1,0, bias=False)\n",
    "        self.proj = nn.Linear(NDF*8, EMBED_DIM)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Extract features\n",
    "        feat_map = self.conv(x)               # [B, NDF*8, 3, 3]\n",
    "        logits = self.out(feat_map).view(x.size(0), -1)  # ✅ apply out() to feat_map instead of x\n",
    "        feat = torch.mean(feat_map, dim=(2,3))            # global average pooling\n",
    "        proj_w = self.proj(feat)\n",
    "        emb = self.label_emb(y)\n",
    "        proj_score = torch.sum(proj_w * emb, dim=1, keepdim=True)\n",
    "        return proj_score + logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa90c53",
   "metadata": {},
   "source": [
    "### 5.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan_stable(\n",
    "    epochs=20, \n",
    "    lr=2e-4, \n",
    "    beta1=0.5, \n",
    "    save_dir=\"cgan_ckpt_stable\",\n",
    "    lambda_gp=10.0\n",
    "):\n",
    "    save = Path(save_dir)\n",
    "    save.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    G = ConditionalGenerator().to(DEVICE)\n",
    "    D = ConditionalDiscriminator().to(DEVICE)\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    \n",
    "    fixed_noise = torch.randn(64, NZ, device=DEVICE)\n",
    "    fixed_labels = torch.tensor([i % NCLASS for i in range(64)], device=DEVICE)\n",
    "\n",
    "    def gradient_penalty(D, real, fake, y):\n",
    "        alpha = torch.rand(real.size(0), 1, 1, 1, device=DEVICE)\n",
    "        interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        d_inter = D(interp, y)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=d_inter,\n",
    "            inputs=interp,\n",
    "            grad_outputs=torch.ones_like(d_inter),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        grad_norm = grad.view(grad.size(0), -1).norm(2, dim=1)\n",
    "        gp = ((grad_norm - 1) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for real, y in dl:\n",
    "            real, y = real.to(DEVICE), y.to(DEVICE)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # ====== Train Discriminator ======\n",
    "            D.zero_grad(set_to_none=True)\n",
    "            z = torch.randn(bs, NZ, device=DEVICE)\n",
    "            fake = G(z, y).detach()\n",
    "\n",
    "            # Label smoothing\n",
    "            real_labels = torch.empty_like(real[:, 0, 0, 0]).uniform_(0.8, 1.0).unsqueeze(1)\n",
    "            fake_labels = torch.empty_like(real[:, 0, 0, 0]).uniform_(0.0, 0.2).unsqueeze(1)\n",
    "\n",
    "            out_real = D(real, y)\n",
    "            out_fake = D(fake, y)\n",
    "            loss_real = F.binary_cross_entropy_with_logits(out_real, real_labels)\n",
    "            loss_fake = F.binary_cross_entropy_with_logits(out_fake, fake_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gp = gradient_penalty(D, real, fake, y) * lambda_gp\n",
    "\n",
    "            loss_D = loss_real + loss_fake + gp\n",
    "            loss_D.backward()\n",
    "            optD.step()\n",
    "\n",
    "            # ====== Train Generator ======\n",
    "            G.zero_grad(set_to_none=True)\n",
    "            z = torch.randn(bs, NZ, device=DEVICE)\n",
    "            fake = G(z, y)\n",
    "            out_fake = D(fake, y)\n",
    "            loss_G = F.binary_cross_entropy_with_logits(out_fake, torch.ones_like(out_fake))\n",
    "            loss_G.backward()\n",
    "            optG.step()\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(f\"Epoch {epoch} Step {step} | D: {loss_D.item():.3f} | G: {loss_G.item():.3f}\")\n",
    "            step += 1\n",
    "\n",
    "        # Save sample grid and checkpoints\n",
    "        with torch.no_grad():\n",
    "            samp = G(fixed_noise, fixed_labels)\n",
    "            grid = vutils.make_grid(samp, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "            vutils.save_image(grid, str(save / f\"epoch_{epoch:03d}.png\"))\n",
    "        torch.save(G.state_dict(), str(save / f\"G_epoch_{epoch:03d}.pt\"))\n",
    "        torch.save(D.state_dict(), str(save / f\"D_epoch_{epoch:03d}.pt\"))\n",
    "\n",
    "    print(\"✅ Stable training finished.\")\n",
    "    return str(save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b9d8c",
   "metadata": {},
   "source": [
    "### 5.2 Inference & Unified API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f143010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_generator(ckpt_path: str):\n",
    "    G = ConditionalGenerator().to(DEVICE)\n",
    "    G.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "    G.eval()\n",
    "    return G\n",
    "\n",
    "def generate_char_with_cgan(G, token: str, size=(48,48)) -> Image.Image:\n",
    "    y = torch.tensor([TOKEN_TO_IDX[token]], device=DEVICE, dtype=torch.long)\n",
    "    z = torch.randn(1, NZ, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = G(z, y)  # [-1,1]\n",
    "    arr = out.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    arr = (arr*0.5 + 0.5) * 255.0\n",
    "    arr = arr.clip(0,255).astype(np.uint8)\n",
    "    pil = Image.fromarray(arr, mode=\"L\").convert(\"RGBA\")\n",
    "    # crude alpha\n",
    "    alpha = pil.convert(\"L\").point(lambda v: 255 if v>15 else 0)\n",
    "    pil.putalpha(alpha)\n",
    "    if size: pil = pil.resize(size, Image.BICUBIC)\n",
    "    return pil\n",
    "\n",
    "def cgan_generate(text: str, G, size=(48,48), augment=True, bg=(255,255,255)):\n",
    "    ims=[]\n",
    "    for ch in text:\n",
    "        token = pick_token_for_char(ch)\n",
    "        im = generate_char_with_cgan(G, token, size=size)\n",
    "        if augment:\n",
    "            im = random_rotation(im, 18)\n",
    "            im = random_perspective(im, 0.12)\n",
    "            im = random_blur(im, 0.35)\n",
    "        ims.append(im)\n",
    "    out = compose_line(ims, spacing=(6,20), vjitter=(-3,6), bg=bg)\n",
    "    out = add_occlusions(out, 0.6, 0.5)\n",
    "    out = random_blur(out, 0.25).convert(\"RGB\")\n",
    "    return out\n",
    "\n",
    "class CaptchaGenerator:\n",
    "    def __init__(self, mode=\"classical\", g_ckpt: Optional[str]=None, bg=(255,255,255)):\n",
    "        self.mode = mode\n",
    "        self.bg = bg\n",
    "        self.G = None\n",
    "        if mode==\"cgan\":\n",
    "            assert g_ckpt and Path(g_ckpt).exists(), \"Provide a valid generator checkpoint\"\n",
    "            self.G = load_generator(g_ckpt)\n",
    "    def generate(self, text: str, size=(48,48), augment=True, save_path: Optional[str]=None) -> Image.Image:\n",
    "        if self.mode==\"classical\":\n",
    "            out = classical_generate(text, size=size, augment=augment, bg=self.bg)\n",
    "        else:\n",
    "            out = cgan_generate(text, self.G, size=size, augment=augment, bg=self.bg)\n",
    "        if save_path:\n",
    "            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            out.save(save_path)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0057fb",
   "metadata": {},
   "source": [
    "## 6. Batch Export Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02cb5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_string(n=5, digits=True, letters=True):\n",
    "    pool = \"\"\n",
    "    if digits: pool += string.digits\n",
    "    if letters: pool += string.ascii_letters\n",
    "    return \"\".join(random.choice(pool) for _ in range(n))\n",
    "\n",
    "def export_synthetic_dataset(out_dir=\"synthetic\", n=100, length_range=(5,6), mode=\"classical\", g_ckpt=None):\n",
    "    gen = CaptchaGenerator(mode=mode, g_ckpt=g_ckpt)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(n):\n",
    "        L = random.randint(*length_range)\n",
    "        s = random_string(L)\n",
    "        img = gen.generate(s, size=(48,48), augment=True)\n",
    "        img.save(out_dir / f\"{s}_{i:05d}.png\")\n",
    "    print(f\"Saved {n} images to {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26e442",
   "metadata": {},
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 tokens/classes.\n",
      "  0 -> 1301 images\n",
      "  1 -> 1333 images\n",
      "  2 -> 1237 images\n",
      "  3 -> 1335 images\n",
      "  4 -> 1300 images\n",
      "  5 -> 1262 images\n",
      "  6 -> 1297 images\n",
      "  7 -> 1253 images\n",
      "  8 -> 1271 images\n",
      "  9 -> 1292 images\n",
      "  A_cap -> 577 images\n",
      "  B_cap -> 659 images\n",
      "  C_cap -> 1175 images\n",
      "  D_cap -> 587 images\n",
      "  E_cap -> 537 images\n",
      "  F_cap -> 610 images\n",
      "  G_cap -> 593 images\n",
      "  H_cap -> 592 images\n",
      "  I_cap -> 863 images\n",
      "  L_cap -> 289 images\n",
      " ...\n",
      "Epoch 1 Step 0 | D 2.127 | G 2.568\n",
      "Epoch 1 Step 200 | D 0.001 | G 7.591\n",
      "Epoch 2 Step 400 | D 0.024 | G 6.528\n",
      "Epoch 2 Step 600 | D 0.600 | G 3.744\n",
      "Epoch 3 Step 800 | D 0.190 | G 5.426\n",
      "Epoch 3 Step 1000 | D 0.452 | G 2.906\n",
      "Epoch 4 Step 1200 | D 0.322 | G 3.950\n",
      "Epoch 4 Step 1400 | D 0.554 | G 1.839\n",
      "Epoch 5 Step 1600 | D 0.460 | G 2.467\n",
      "Epoch 5 Step 1800 | D 0.865 | G 3.732\n",
      "Epoch 6 Step 2000 | D 0.435 | G 2.362\n",
      "Epoch 6 Step 2200 | D 0.351 | G 2.777\n",
      "Epoch 7 Step 2400 | D 0.252 | G 3.290\n",
      "Epoch 7 Step 2600 | D 0.268 | G 3.388\n",
      "Epoch 8 Step 2800 | D 0.271 | G 2.327\n",
      "Epoch 8 Step 3000 | D 0.485 | G 1.972\n",
      "Epoch 9 Step 3200 | D 0.312 | G 3.010\n",
      "Epoch 9 Step 3400 | D 0.275 | G 3.099\n",
      "Epoch 10 Step 3600 | D 0.249 | G 4.587\n",
      "Epoch 10 Step 3800 | D 0.198 | G 3.285\n",
      "Epoch 11 Step 4000 | D 0.271 | G 3.036\n",
      "Epoch 12 Step 4200 | D 0.284 | G 4.907\n",
      "Epoch 12 Step 4400 | D 0.173 | G 3.845\n",
      "Epoch 13 Step 4600 | D 0.172 | G 3.871\n",
      "Epoch 13 Step 4800 | D 0.198 | G 4.007\n",
      "Epoch 14 Step 5000 | D 0.188 | G 5.230\n",
      "Epoch 14 Step 5200 | D 0.789 | G 5.405\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Adjust epochs to your dataset size; e.g. 10–30 epochs for small sets\n",
    "ckpt_dir = train_cgan_stable(\n",
    "    epochs=20,\n",
    "    lr=2e-4,\n",
    "    beta1=0.5,\n",
    "    save_dir=\"cgan_ckpt_stable\"\n",
    ")\n",
    "\n",
    "print(\"✅ Model saved under:\", ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generator checkpoint\n",
    "from pathlib import Path\n",
    "best_ckpt = Path(\"cgan_ckpt/G_epoch_020.pt\")\n",
    "G = load_generator(str(best_ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d25e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = CaptchaGenerator(mode=\"cgan\", g_ckpt=str(best_ckpt))\n",
    "img = gen.generate(\"X5vUa\", save_path=\"samples/example_cgan.png\")\n",
    "show_image(img, \"cGAN CAPTCHA via CaptchaGenerator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190445de",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_classical = CaptchaGenerator(mode=\"classical\")\n",
    "img1 = gen_classical.generate(\"A8dP3\")\n",
    "img2 = gen.generate(\"A8dP3\")\n",
    "\n",
    "show_image(img1, \"Classical\")\n",
    "show_image(img2, \"cGAN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6c3de",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Notes\n",
    "- If some classes are scarce, consider oversampling or weighted sampling during cGAN training.\n",
    "- Increase `IMG_SIZE` to 64 and adjust conv stack for more detail.\n",
    "- You can add elastic warps, shadows, curved baselines in the **whole-canvas** step.\n",
    "- For fully end-to-end sequence generators (string→image), consider diffusion or transformer-based renderers later; \n",
    "  here we keep it robust and practical for your dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4243_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
